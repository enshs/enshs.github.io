---
layout: single
title: "RNN 소개"
categories:
  - Deep Learning
tags:
  - NLP
  - nltk
  - python
use_math: true
comments: true
---


# 순환신경망 (RNN)

본 글은 RNN의 소개 글로서 <https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch>를 번역한 것입닏. 

다음은 전통적 신경망(traditional feed-forward neural networks)과 RNN사이의 차이를 나타낸 것입니다.

![참조그림1](https://blog.floydhub.com/content/images/2019/04/Slide3-1.jpg)

주요한 차이점은 모델에서 입력 데이터를 가져오는 방법에 있습니다. 
- feed-forward neutral networks : 고정된 양의 입력데이터를 받아 고정된 양을 출력
- RNN: 모든 입력된 데이터를 한번에 소비하지 않습니다. 대신에 한번에 하나씩 순서대로 가져옵니다. 각 단계에서 RNN은 출력을 생성하기 전에 일련의 계산을 수행. 숨겨진 상태로 알려진(hidden layer) 출력은 다음 시퀀스 입력과 결합되어 다른 출력을 생성. 이 과정은 모델이 완료되도록 프로그래밍되거나 시퀀스가 끝날때 까지 계속됩니다.

**RNN 출력**

각 시간 단계의 계산은 숨겨진 상태의 형태로 이전 시간 단계의 컨텍스트를 고려합니다. 즉, 이전 입력으로부터의 출력을 사용할수 있다는 것은 RNN의 핵심입니다. 위 그림에서 각 시간 단계에서 사용되는 RNN 셀은 동일한 셀을 재이용하는 것입니다. 

각 RNN셀 후에 출력이 이루어집니다. 즉, 다중 출력이 이루어집니다. 이 다중 출력에 최종적으로 사용되는 출력이 무엇인지는 이 모델의 적용 사례에 따라 다릅니다. 예를 들어 분류 작업을 위해 사용되는 경우 모든 입력이 전달된 후 최종 출력 하나만이 필요합니다. (확률점수 벡터) 이전의 문자 또는 단어를 기반으로 텍스트 생성을 수행하는 경우 모든 단일 시간 단계에서 출력이 필요합니다. 

![참조그림2](https://blog.floydhub.com/content/images/2019/04/karpathy.jpeg)

전체 프로세스에서 단일 출력만 필요한 경우로 시퀀스의 마지막 RNN 셀에서 생성된 출력만을 가져옵니다. 이 최종출력은 모든 이전 셀을 통해 이미 계산된 과정을 포함한 것입니다. 즉, 최종결과가 이전의 모든 계산 및 입력에 의존한다는 것을 의미합니다. 

![참조그림3](https://blog.floydhub.com/content/images/2019/04/Slide6.jpg)

중간단계의 출력정보가 필요한 경우 각 단계에서 은닉 상태에서 정보를 가져올 수 있습니다. 생성된 출력은 필요한 경우 다음 단계에서 모델로 피드백 될 수 있습니다.

![참조그림4](https://blog.floydhub.com/content/images/2019/04/Slide7.jpg)

이외에서 다양한 출력 유형을  생성할 수 있습니다. 다음 그림은 모든 입력이 통과된 후에 만 출력이 생성되는 생성되는 시퀀스 대 시퀀스 변환을 나타낸 것입니다. 

![참조그림5](https://blog.floydhub.com/content/images/2019/04/Slide8.jpg)

시퀀스 운영 체제는 처음부터 고정된 수의 계산 단계에 의해 결정되는 고정 네트워크에 비해 훨씬 강력하므로 더 지능적인 시스템을 구축가능합니다. 더욱이, RNN은 새로운 상태 벡터를 생성하기 위해 고정된(그러나 학습된) 함수와 함께 입력 벡터와 상태 벡터를 결합합니다. 이것은 프로그래밍 용어로 특정 입력과 일부 내부 변수를 사용하여 고정 프로그램을 실행하는 것으로 해석될 수 있습니다. 이런 식으로 보면 RNN은 본질적으로 프로그램을 설명합니다. 사실, RNN은 임의의 프로그램(적절한 가중치 사용)을 시뮬레이션할 수 있다는 점에서 Turing-Complete인 것으로 알려져 있습니다. 

**RNN의 작동 방식**

숨겨진 상태와 출력을 생성하기 위해 RNN의 셀이 수행해야 하는 몇 가지 기본적인 계산을 살펴보겠습니다.

$$\begin{align}  h_t &= F(h_{t-1}, i_t)\\
h:& \text{hidden} \\  i:& \text{input} \end{align}$$

첫 번째 단계에서 숨겨진 상태는 일반적으로 시퀀스의 첫 번째 입력과 함께 RNN 셀에 공급될 수 있도록 0의 행렬로 제공됩니다. 가장 단순한 RNN에서 숨겨진 상태와 입력 데이터는 Xavier 또는 Kaiming과 같은 체계를 통해 초기화된 가중치 행렬로 곱해집니다. 이러한 곱셈의 결과는 비선형성을 도입하기 위해 활성화 함수(예: tanh 함수)를 통해 전달됩니다.
<br>	
$$\begin{align}h_t&=\text{tanh}(w_h \cdot h_{t-1} + w_i \cdot i_t)\\ 
w_h:& \text{hidden에서의 가중치} \\  w_i:& \text{input에서의 가중치} \end{align}$$
<br>	
또한 각 시간 단계가 끝날 때 출력이 필요한 경우 선형 레이어를 통해 방금 생성한 은닉 상태를 전달하거나 원하는 결과 모양을 얻기 위해 다른 가중치 행렬을 곱할 수 있습니다. <br>	

$$\begin{align}o_t &= w_o \cdot h_t\\  o_t :& \text{t 시점에서의 출력} \\ w_o:& \text{출력에 적용되는 가중치}	\end{align}$$
<br>
생성한 은닉 상태는 다음 입력과 함께 RNN  셀로 전달됩니다. 이 과정은 입력이 부족하거나 모델의 출력 생성이 중지되도록 프로그래밍될 때 까지 계속 됩니다.   

이 단순 방법을 수정한 보다 복잡한 RNN 구조 가지는 LSTM, GRU등의 방법이 존재합니다. 

**RNN 계산**
RNN 계산 방식은 간단합니다. 입력 벡터 x를 받아들이고 출력 벡터 y를 제공합니다. 그러나 결정적으로 이 출력 벡터의 내용은 방금 입력한 입력뿐만 아니라 과거에 입력한 전체 입력 기록의 영향을 받습니다. 클래스로 작성된 RNN의 API는 단일 단계 함수로 구성됩니다.

```
rnn=RNN()
y=rnn.step()
```
RNN 클래스에는 단계가 호출될 때마다 업데이트되는 내부 상태가 있습니다. 가장 단순한 경우 이 상태는 단일 은닉 벡터 h로 구성됩니다. 다음은 바닐라 RNN의 단계 함수 구현입니다.
```
class RNN:
    ...
    def step(self, x):
        # 은닉상태(hidden state)에서 업데이트 
        self.h=np.tanh(np.dot(self.w_hh, self.h)+np.dot(self.w_xh, x))
        # 출력벡터
        y=np.dot(self.w_hy, self.h)
        return y
```
위의 내용은 바닐라 RNN의 정방향 패스를 지정합니다. 이 RNN의 매개변수는 $W_{hh}, W_{xh}, W_{hy}$의 세 가지 행렬입니다. 은닉 상태 self.h는 0 벡터로 초기화됩니다. np.tanh 함수는 활성화를 [-1, 1] 범위로 압축하는 비선형성을 구현합니다. 이것이 어떻게 작동하는지 간략하게 주목하십시오. tanh 내부에는 두 가지 용어가 있습니다. 하나는 이전 은닉 상태를 기반으로 하고 다른 하나는 현재 입력을 기반으로 합니다. numpy에서 np.dot는 행렬 곱셈입니다. 두 중간체는 덧셈과 상호 작용한 다음 tanh에 의해 새 상태 벡터가 됩니다. 수학 표기법이 더 편하다면 은닉 상태 업데이트를 다음과 같이 작성할 수 있습니다.

$$h_t=tanh(W_{hh}h_{t−1}+W_{xh}x_t)$$

여기서 tanh는 요소별로 적용됩니다. RNN의 행렬을 난수로 초기화하고 손실함수를 고려하여 최적의 행렬을 찾습니다. 다음과 같이 2계층 순환 네트워크를 형성할 수 있습니다.

```
y1=rnn1.step(x)
y=rnn2.step(y1)
```

즉, 두 개의 개별 RNN이 있습니다. 하나의 RNN은 입력 벡터를 수신하고 두 번째 RNN은 첫 번째 RNN의 출력을 입력으로 수신합니다. 이러한 RNN 중 어느 것도 알지 못하거나 관심을 두지 않는다는 점을 제외하고는 모두 들어오고 나가는 벡터와 backpropagation 동안 각 모듈을 통해 흐르는 약간의 그라디언트일 뿐입니다.


**학습과 역전파** 

다른 형태의 신경망과 마찬가지로 RNN 모델은 일련의 입력이 전달된 후 정확하고 원하는 출력을 생성하기 위해 훈련되어야 합니다. 

![참조그림6](https://blog.floydhub.com/content/images/2019/04/rnn-bptt-with-gradients.png)

훈련하는 동안 훈련 데이터의 각 부분에 대해 해당하는 정답 레이블을 가지거나 단순히 모델이 출력하기를 원하는 "정답"을 입력합니다. 물론, 모델을 통해 입력 데이터를 전달하는 처음 몇 번에는 이러한 정답과 동일한 출력을 얻지 못할 것입니다. 그러나 이러한 출력을 받은 후 훈련 중에 수행할 작업은 모델의 출력이 정답에서 얼마나 떨어져 있는지 측정하는 해당 프로세스의 손실을 계산하는 것입니다. 이 손실을 사용하여 역전파에 대한 손실 함수의 기울기를 계산할 수 있습니다.

역전파로부터 계산된 그래디언트를 사용하여 모델의 가중치를 적절하게 업데이트하여 입력 데이터에 적용하여 보다 정확한 결과를 생성하도록 할 수 있습니다. 여기서 가중치는 순방향 과정 동안 입력 데이터와 숨겨진 상태(hidden state)를 곱한 가중치 행렬을 나타냅니다. 기울기를 계산하고 가중치를 업데이트하는 이 전체 프로세스를 역전파라고 합니다. 순전파 과정과 결합하여 역전파가 계속 반복되므로 가중치 행렬 값이 데이터 패턴을 선택하기 위해 수정될 때마다 모델이 더 정확해질 수 있습니다.

각 RNN 셀이 그래픽과 같이 다른 가중치를 사용하는 것처럼 보일 수 있지만 실제로 모든 가중치는 해당 RNN 셀이 프로세스 전반에 걸쳐 본질적으로 재사용되는 것과 동일합니다. 따라서 입력 데이터와 이월된 은닉 상태만 각 시간 단계에서 고유합니다. 

RNN은 시퀀스 모델 즉, 입력과 출력을 시퀀스 단위로 처리하는 모델
<br>
$$\text{입력(시퀀스)} \rightarrow \text{출력(시퀀스)}$$
<br>
일반적인 신경망은 다음과 같이 은닉층의 결과는 출력층으로만 진행하는 모델로 Feed forward  neural network입니다. 반면에 RNN은 은닉층의 활성화 함수를 통한 결과를 출력층으로 전달하면서 다시 은닉층의 계산값으로 입력합니다.
<br>
![참조그림7](https://blog.floydhub.com/content/images/2019/04/Slide9.jpg)
<br>
$$ 𝑋_𝑡 \rightarrow \text{cell} \rightarrow 𝑌_𝑡$$
X: 입력층의 입력벡터
Y: 출력층의 출력벡터
Cell: 은닉층으로 활성함수에 의한 결과를 내보내는 노드, 이전의 값을 기억하는 일종의 메모리 역할을 수행하는 메모리셀 또는 RNN 셀

메모리 셀에서 은닉상태를 계산하는 식은 다음과 같이 정의 
<br>   
$$h_t =tanh(W_xX_t+W_h h_{t-1} +b)$$ 
<br>
다음은 간단한 의사코드(pseudo code)입니다.    

다음은 간단한 의사코드(pseudo code)입니다.    

```
hidden_state_t =0  #초기화, 초기 은닉상태를 0으로 정의     
for input_t in input_length:   #각 시점마다 입력 받음     
    output_t = tanh(input_t, hidden_state_t) #각 시점에 대해 은닉상태를 가지고 연산 
    hidden_state_t =output_t #계산결과는 현재 시점의 은닉상태가 됨 
```

- hidden_tate_t: t 시점의 은닉 상태  
- input_length: 입력 데이터의 길이, 입력데이터의 길이는 총 시점의 수(timesteps)가 됩니다. 
- input_t: t 시점의 입력값

각 메모리 셀은 각 시점마다 input_t와 hidden_tate_t를 적용하여 활성함수(tanh)에 의해 현 시점의 hidden_state_t를 계산합니다.        

hidden은 각 시점의 가중치가 되고 기본 연산은 Wx입니다. 그러므로 W의 차원은 입력의 행의 수와 같아야 합니다.  

예) 전달된 단어 또는 몇 글자를 기반으로 문장을 완성하는 모델을 구축할 것입니다.

![참조그림8](https://blog.floydhub.com/content/images/2019/04/Slide4.jpg)

모델에 단어가 입력되고 문장의 다음 문자가 무엇인지 예측합니다. 이 과정은 원하는 길이의 문장을 생성할 때까지 반복됩니다.

모델의 학습 방법을 나타내기 위해 몇개의 짧은 문장을 정의할 것입니다. 이 구현이 수행하는 프로세스는 다음과 같습니다. 

![참조그림9](https://blog.floydhub.com/content/images/2019/04/Slide5.jpg)
